{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Info\n","\n","\n","Here I will develop the simple densely connect model which will serve as a baseline. The idea is to create a model that can train in about 10 minutes and that provides with a reasonable baseline for more complex models."]},{"cell_type":"markdown","metadata":{},"source":["# Version v1.0\n","\n","TODO:\n","\n","- napravit vector embedding sensor_id-a prema koordinatama sensora, a ne prema embed_dim. Tako ćeš imat samo 3 koordinate. Možeš svim tim vektorima oduzet centar mase. Pitanje: je li neutrino u datasetu nužno prolazi kroz ishodište?\n","- hendlat vrijeme kao kumulativnu varijablu? \"time (int): the time of the pulse in nanoseconds in the current event time window. The absolute time of a pulse has no relevance, and only the relative time with respect to other pulses within an event is of relevance.\"\n","- na kraju ti je dimenzija input vektora (B, T, 3 + 1 + 1), 3 za lokaciju senzora (xyz), jedan za naboj i jedan za kumulativno vrijeme (koliko je vremena prošlo od početka). Možda možeš dodat još i jednu dimenziju, a to je auxiliary, dakle to bi bilo (B, T, 6)\n","- It's very important to remove the center of mass from the points to correctly estimate the angles!\n"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-02-26T16:51:24.738619Z","iopub.status.busy":"2023-02-26T16:51:24.738235Z","iopub.status.idle":"2023-02-26T16:51:24.744168Z","shell.execute_reply":"2023-02-26T16:51:24.743034Z","shell.execute_reply.started":"2023-02-26T16:51:24.738588Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/ubuntu/miniconda3/envs/pytorch/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import random\n","import math\n","import wandb\n","import gc"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-02-26T16:51:24.755044Z","iopub.status.busy":"2023-02-26T16:51:24.754771Z","iopub.status.idle":"2023-02-26T16:51:24.762258Z","shell.execute_reply":"2023-02-26T16:51:24.761275Z","shell.execute_reply.started":"2023-02-26T16:51:24.755019Z"},"trusted":true},"outputs":[],"source":["\n","LOG_WANDB = True\n","# hyperparameters\n","n_opt_distance=3000000 # number of optimizations for distance loss\n","n_opt_angular=0 # number of optimizations for angular loss\n","max_iters = n_opt_distance + n_opt_angular # number of iterations\n","change_batch_every = 1000 # change batch of data every change_batch_every steps\n","device =   'cuda' if torch.cuda.is_available() else 'cpu'\n","evaluate_every_step = 1024\n","eval_iters = 32 # number of batches to process for evaluation\n","\n","grad_clip = False # enable gradient clipping\n","grad_clip_val = 1e-8 # gradient clipping value\n","\n","## network hyperparameters\n","batch_size= 256\n","gradient_accumulation_steps = 512 // batch_size # 256 // batch_size # used to simulate larger batch sizes\n","learning_rate = 1e-4 / gradient_accumulation_steps\n","block_size = 128 # maximum context length\n","n_embd = 196\n","n_layers = 3\n","num_heads = 14\n","dropout = 0.2\n","ffwd_scale = 4\n","# --------------------\n","## \n","num_bins = 64\n","\n","print_each = 10\n","print_model_each = 1000\n","\n","clip_charge = 5\n","charge_rescale = 5\n","max_t = 16000.\n","max_xyz = 500\n","\n","\n","torch.manual_seed(1337);"]},{"cell_type":"markdown","metadata":{},"source":["# Load sensor geometry"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-02-26T16:51:24.774882Z","iopub.status.busy":"2023-02-26T16:51:24.772565Z","iopub.status.idle":"2023-02-26T16:51:24.788231Z","shell.execute_reply":"2023-02-26T16:51:24.787185Z","shell.execute_reply.started":"2023-02-26T16:51:24.774855Z"},"trusted":true},"outputs":[{"data":{"text/plain":["([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 5160)"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["df_sensor_geometry = pd.read_csv(\"../../sensor_geometry.csv\")\n","sensor_ids = sorted(list(set(df_sensor_geometry[\"sensor_id\"].to_list())))\n","sensor_ids[:10], len(sensor_ids)"]},{"cell_type":"markdown","metadata":{},"source":["## Load metadata\n","\n","This code defines a class called `EventDataLoader` that is responsible for loading and preprocessing data for a neutrino particle detection problem. \n","\n","Upon initialization, the class loads the training and test metadata files in Parquet format and splits the training data into a training and validation set. It also sets some internal variables for tracking the current batch and the last batch switch time. Additionally, it loads the events from Parquet files into memory and preprocesses them using several helper functions.\n","\n","The main functions in the class are `get_single_event` and `get_xy`. The former takes as input the indices of the first and last pulse for a given event and a split type (train, test, or eval), preprocesses the event data, and returns the processed data as tensors. The `get_xy` function returns a batch of batch_size preprocessed events and their associated target values for either the training or test split.\n","\n","The class also has a method `change_event_batch` that shuffles the current batch of events and associated metadata and reloads the new batch of events from disk. This function is called every change_batch_every iterations of the get_xy function to allow for iterating through multiple batches of data."]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-02-26T16:51:24.795375Z","iopub.status.busy":"2023-02-26T16:51:24.795089Z","iopub.status.idle":"2023-02-26T16:51:24.823030Z","shell.execute_reply":"2023-02-26T16:51:24.822182Z","shell.execute_reply.started":"2023-02-26T16:51:24.795351Z"},"trusted":true},"outputs":[],"source":["class EventDataLoader:\n","    def __init__(self):\n","        self.change_batch_every = change_batch_every\n","        # train-test split in meta\n","        self.batch_ids = {\n","            \"train\": np.arange(1, 660),\n","            \"test\": np.array([660]),\n","        }\n","        self.batch_id_current = {\n","            'train': 0,\n","            'test': 0\n","        }\n","        self.meta_data_current = {\n","            \"train\": self.shuffle_metadata_batch(\"train\"),\n","            \"test\": self.shuffle_metadata_batch(\"test\"),\n","            \"eval\": pd.read_parquet(\"../../test_meta.parquet\").reset_index(drop=True)\n","        }\n","        # events\n","        self.events = {\n","            \"train\": self.load_batch_events(\"train\"),\n","            \"test\": self.load_batch_events(\"test\"),\n","            \"eval\": pd.read_parquet(\"../../test/batch_661.parquet\").reset_index(drop=True)\n","        }\n","        self.counter=0 # used to keep track of the current batch\n","    \n","    # Helper functions\n","    def shuffle_metadata_batch(self, split):\n","        \n","        assert split in [\"train\", \"test\"]\n","        \n","        if split in [\"train\", \"test\"]:\n","            # take random element from self.batch_ids[split]\n","            random_batch_id = np.random.choice(self.batch_ids[split])\n","            self.batch_id_current[split] = random_batch_id\n","        else:\n","            random_batch_id = 660\n","            self.batch_id_current[split] = 660\n","        return pd.read_parquet(f\"/home/ubuntu/neutrino/dataset-2/train_meta_{random_batch_id}.parquet\")\n","    \n","    \n","    def load_batch_events(self,split):\n","        \"\"\" load a batch of events from the parquet file \"\"\"\n","        \n","        \n","        special_dir = \"train\" if split in [\"train\", \"test\"] else \"test\"\n","        batch_id = self.batch_id_current[split]\n","        return pd.read_parquet(f\"../../{special_dir}/batch_{batch_id}.parquet\").reset_index()\n","        \n","    \n","    # --- Main functions ---\n","    \n","\n","\n","    def change_event_batch(self):\n","        \"\"\" change both the train and test event batches \"\"\"\n","        \n","        self.meta_data_current = {\n","            \"train\":self.shuffle_metadata_batch(\"train\"),\n","            \"test\":self.shuffle_metadata_batch(\"test\"),\n","        }\n","        self.events[\"train\"] = self.load_batch_events(\"train\")\n","        self.events[\"test\"] = self.load_batch_events(\"test\")\n","        self.counter = 0\n","    \n","    \n","    # def get_single_event(self, first_pulse_index, last_pulse_index, split):\n","    #     \"\"\" get a single event from the dataframe (train or test) \"\"\"\n","\n","    #     assert split in [\"train\", \"test\", \"eval\"], \"split must be either 'train', 'test' or 'eval'\"\n","\n","    #     # get the event\n","    #     event = self.events[split].iloc[first_pulse_index:last_pulse_index+1]\n","        \n","        \n","        \n","    #     # merge event with df_sensor_geometry using sensor_id to get x, y, z\n","    #     event = pd.merge(event, df_sensor_geometry, on=\"sensor_id\")\n","\n","    #     # preprocess xyz\n","    #     event[\"x\"] = (event[\"x\"] - np.average(event[\"x\"])) / max_xyz\n","    #     event[\"y\"] = (event[\"y\"] - np.average(event[\"y\"])) / max_xyz\n","    #     event[\"z\"] = (event[\"z\"] - np.average(event[\"z\"])) / max_xyz\n","\n","    #     # pad with zeros if the event is smaller than the block size\n","    #     event_size = len(event)\n","    #     if event_size < block_size:\n","    #         event = event.append(pd.DataFrame(np.zeros((block_size - event_size, len(event.columns))), columns=event.columns))\n","    #     else:\n","    #         # event = event[:block_size]\n","    #         # take random block_size elements from the event\n","    #         event = event.sample(n=block_size, random_state=1337)\n","\n","    #     xyz = event[['x', 'y', 'z']].values\n","    #     # preprocess time\n","    #     time = event[\"time\"].values / max_t\n","    #     # time = np.cumsum(time)\n","    #     time[event_size:] = -1\n","\n","    #     # preprocess charge\n","    #     charge = event[\"charge\"].values\n","    #     charge_clipped = charge > clip_charge\n","    #     charge = np.clip(charge, 0, clip_charge)\n","    #     charge = charge / charge_rescale\n","    #     charge[event_size:] = -1 # if there's no data, set the charge value to -1.\n","        \n","    #     # preprocess auxiliary\n","    #     auxiliary = np.array(event[\"auxiliary\"].values, dtype=np.float32)\n","    #     auxiliary[event_size:] = -1 # if there's no data, set the auxiliary value to -1.\n","\n","    #     has_event = np.zeros(block_size, dtype=np.float32)\n","    #     has_event[:event_size] = 1\n","        \n","    #     # convert to tensors\n","    #     xyz = torch.tensor(xyz, dtype=torch.float32)\n","    #     time = torch.tensor(time, dtype=torch.float32)\n","    #     charge = torch.tensor(charge, dtype=torch.float32)\n","    #     charge_clipped = torch.tensor(charge_clipped, dtype=torch.float32)\n","    #     auxiliary = torch.tensor(auxiliary, dtype=torch.float32)\n","    #     has_event = torch.tensor(has_event, dtype=torch.float32)\n","\n","    #     return {\"xyz\": xyz, \"time\": time, \"charge\": charge, \"charge_clipped\": charge_clipped, 'auxiliary':auxiliary, 'has_event':has_event, 'event_size': torch.tensor(event_size)}\n","\n","    def get_single_event(self, first_pulse_index, last_pulse_index, split):\n","        \"\"\" get a single event from the dataframe (train or test) \"\"\"\n","        # get the event\n","        event = self.events[split].iloc[first_pulse_index:last_pulse_index+1]\n","        \n","        \n","        \n","        # merge event with df_sensor_geometry using sensor_id to get x, y, z\n","        event = pd.merge(event, df_sensor_geometry, on=\"sensor_id\")\n","        \n","        \n","        event_size = len(event)\n","        \n","        if event_size > block_size:\n","            # take random block_size elements from the event\n","            event = event.sample(n=block_size, random_state=1337)\n","            # event = event[:block_size]\n","\n","            \n","        has_event = torch.zeros(block_size, dtype=torch.long)\n","        has_event[:event_size] = 1\n","        \n","        \n","        # preprocess xyz\n","        event_xyz = event[['x', 'y', 'z']].to_numpy() / max_xyz\n","        \n","        # pad with zeros if the event is smaller than the block size\n","        if event_size < block_size:\n","            padding_rows = block_size - event_size\n","            padding = np.zeros((padding_rows, 3))\n","            event_xyz = np.vstack((event_xyz, padding))\n","\n","        event_xyz_avg = event_xyz.copy()\n","        event_xyz_avg[has_event==1] = (event_xyz[has_event==1] - event_xyz[has_event==1].mean(axis=0, keepdims=True)) / event_xyz[has_event==1].std(axis=0, keepdims=True)\n","        event_xyz_avg[has_event == 0] = 0 \n","        event_xyz[has_event == 0] = 0 \n","            \n","        # preprocess time\n","        time = torch.tensor(event[\"time\"].to_numpy() / max_t, dtype=torch.float32)\n","        time = (time - time.mean()) / time.std()\n","        if event_size < block_size:\n","            time = torch.cat((time, torch.full((block_size - event_size,), 0, dtype=torch.float32)))\n","\n","        # preprocess charge\n","        charge = torch.tensor(event[\"charge\"].to_numpy(), dtype=torch.float32)\n","        charge_clipped = (charge > clip_charge).float()\n","        charge = torch.clamp(charge, 0, clip_charge) / 2\n","        charge = (charge - charge.mean()) / charge.std()\n","        if event_size < block_size:\n","            charge = torch.cat((charge, torch.full((block_size - event_size,), 0, dtype=torch.float32)))\n","            charge_clipped = torch.cat((charge_clipped, torch.full((block_size - event_size,), 0, dtype=torch.float32)))\n","\n","        # preprocess auxiliary\n","        auxiliary = torch.tensor(event[\"auxiliary\"].to_numpy(), dtype=torch.float32)\n","        if event_size < block_size:\n","            auxiliary = torch.cat((auxiliary, torch.full((block_size - event_size,), 0, dtype=torch.float32)))\n","\n","        \n","        out = {\n","            \"xyz\": torch.tensor(event_xyz_avg, dtype=torch.float32), \n","            # \"xyz\": torch.tensor(event_xyz, dtype=torch.float32), \n","            \"time\": time, \n","            \"charge\": charge,\n","            \"charge_clipped\": charge_clipped,\n","            'auxiliary':auxiliary, \n","            'has_event':has_event.float()\n","        }\n","\n","        \n","        out = {\n","            \"xyz\": torch.tensor(event_xyz_avg, dtype=torch.float32), \n","            # \"xyz\": torch.tensor(event_xyz, dtype=torch.float32), \n","            \"time\": time, \n","            \"charge\": charge,\n","            \"charge_clipped\": charge_clipped,\n","            'auxiliary':auxiliary, \n","            'has_event':has_event.float()\n","        }\n","        \n","        return out\n","\n","    def get_xy(self, split):\n","        assert split in [\"train\", \"test\"], \"split must be either 'train' or 'test'\"\n","\n","        df_meta_sample = self.meta_data_current[split].sample(n=batch_size).reset_index(drop=True)\n","        \n","\n","        # in dataframe df_meta_sample, loop over first_pulse_index and last_pulse_index columns, and slice the corresponding rows from df_events_batch\n","        first_pulse_indices = df_meta_sample[\"first_pulse_index\"].to_list()\n","        last_pulse_indices = df_meta_sample[\"last_pulse_index\"].to_list()\n","\n","        # preprocessed data as tensors\n","        preprocessed_data = [self.get_single_event(first_pulse_index, last_pulse_index, split) for first_pulse_index, last_pulse_index in zip(first_pulse_indices, last_pulse_indices)]\n","\n","        # create batch of preprocessed data\n","        xyz_batch = torch.stack([data[\"xyz\"] for data in preprocessed_data])\n","        time_batch = torch.stack([data[\"time\"] for data in preprocessed_data])\n","        charge_batch = torch.stack([data[\"charge\"] for data in preprocessed_data])\n","        charge_clipped_batch = torch.stack([data[\"charge_clipped\"] for data in preprocessed_data])\n","        auxiliary_batch = torch.stack([data[\"auxiliary\"] for data in preprocessed_data])\n","        has_event_batch = torch.stack([data[\"has_event\"] for data in preprocessed_data])\n","        \n","\n","        # Targets\n","        theta_target = np.squeeze(np.array(df_meta_sample[[\"zenith\"]].values, dtype=np.float32))\n","        phi_target = np.squeeze(np.array(df_meta_sample[[\"azimuth\"]].values, dtype=np.float32))\n","        # y = np.array([\n","        #     np.sin(theta_target)*np.cos(phi_target), \n","        #     np.sin(theta_target)*np.sin(phi_target), \n","        #     np.cos(theta_target)], dtype=np.float32)\n","        # y = torch.tensor(y.T)\n","        # y[:,0] should be azimuth, y[:,1] should be zenith\n","        y = torch.tensor(np.array([phi_target, theta_target]).T, dtype=torch.float32)\n","        \n","\n","        \n","        # send them to the device\n","        xyz_batch = xyz_batch.to(device)\n","        time_batch = time_batch.to(device)\n","        charge_batch = charge_batch.to(device)\n","        charge_clipped_batch = charge_clipped_batch.to(device)\n","        auxiliary_batch = auxiliary_batch.to(device)\n","        has_event_batch = has_event_batch.to(device)\n","        y = y.to(device)\n","\n","        self.counter += 1\n","        if self.counter == self.change_batch_every:\n","            self.change_event_batch()\n","            self.counter = 0\n","        \n","        \n","        X = {\n","            \"xyz\": xyz_batch,\n","            \"time\": time_batch,\n","            \"charge\": charge_batch,\n","            \"charge_clipped\": charge_clipped_batch,\n","            \"auxiliary\": auxiliary_batch,\n","            \"has_event\": has_event_batch\n","        }\n","        \n","        return X, y\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-02-26T16:51:24.834706Z","iopub.status.busy":"2023-02-26T16:51:24.834452Z","iopub.status.idle":"2023-02-26T16:51:30.731177Z","shell.execute_reply":"2023-02-26T16:51:30.730192Z","shell.execute_reply.started":"2023-02-26T16:51:24.834683Z"},"trusted":true},"outputs":[],"source":["data_loader = EventDataLoader()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-02-26T16:51:30.733363Z","iopub.status.busy":"2023-02-26T16:51:30.732988Z","iopub.status.idle":"2023-02-26T16:51:31.063716Z","shell.execute_reply":"2023-02-26T16:51:31.062806Z","shell.execute_reply.started":"2023-02-26T16:51:30.733328Z"},"trusted":true},"outputs":[],"source":["# # Let's check how one example looks like\n","# X, y = data_loader.get_xy(split=\"train\")\n","# X['xyz'][:3], X['time'][:3], X['charge'][:3], X['auxiliary'][:3], X['event_size'][:3], y[:3]"]},{"cell_type":"markdown","metadata":{},"source":["# Angular Loss function"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-02-26T16:51:31.089962Z","iopub.status.busy":"2023-02-26T16:51:31.089645Z","iopub.status.idle":"2023-02-26T16:51:31.103057Z","shell.execute_reply":"2023-02-26T16:51:31.102096Z","shell.execute_reply.started":"2023-02-26T16:51:31.089933Z"},"trusted":true},"outputs":[],"source":["def angular_dist_score(az_true, zen_true, az_pred, zen_pred):\n","    '''\n","    calculate the MAE of the angular distance between two directions.\n","    The two vectors are first converted to cartesian unit vectors,\n","    and then their scalar product is computed, which is equal to\n","    the cosine of the angle between the two vectors. The inverse \n","    cosine (arccos) thereof is then the angle between the two input vectors\n","    \n","    Parameters:\n","    -----------\n","    \n","    az_true : float (or array thereof)\n","        true azimuth value(s) in radian\n","    zen_true : float (or array thereof)\n","        true zenith value(s) in radian\n","    az_pred : float (or array thereof)\n","        predicted azimuth value(s) in radian\n","    zen_pred : float (or array thereof)\n","        predicted zenith value(s) in radian\n","    \n","    Returns:\n","    --------\n","    \n","    dist : float\n","        mean over the angular distance(s) in radian\n","    '''\n","    \n","    if not (torch.all(torch.isfinite(az_true))  and\n","            torch.all(torch.isfinite(zen_true)) and\n","            torch.all(torch.isfinite(az_pred)) and\n","            torch.all(torch.isfinite(zen_pred))\n","           ):\n","        raise ValueError(\"All arguments must be finite\")\n","    \n","    # pre-compute all sine and cosine values\n","    sa1 = torch.sin(az_true)\n","    ca1 = torch.cos(az_true)\n","    sz1 = torch.sin(zen_true)\n","    cz1 = torch.cos(zen_true)\n","    \n","    sa2 = torch.sin(az_pred)\n","    ca2 = torch.cos(az_pred)\n","    sz2 = torch.sin(zen_pred)\n","    cz2 = torch.cos(zen_pred)\n","    \n","    # scalar product of the two cartesian vectors (x = sz*ca, y = sz*sa, z = cz)\n","    scalar_prod = sz1*sz2*(ca1*ca2 + sa1*sa2) + (cz1*cz2)\n","    \n","    # scalar product of two unit vectors is always between -1 and 1, this is against nummerical instability\n","    # that might otherwise occure from the finite precision of the sine and cosine functions\n","    scalar_prod =  torch.clamp(scalar_prod, -1, 1)\n","    \n","    # convert back to an angle (in radian)\n","    return torch.abs(torch.acos(scalar_prod))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-02-26T16:51:31.128414Z","iopub.status.busy":"2023-02-26T16:51:31.127131Z","iopub.status.idle":"2023-02-26T16:51:31.174840Z","shell.execute_reply":"2023-02-26T16:51:31.173921Z","shell.execute_reply.started":"2023-02-26T16:51:31.128377Z"},"trusted":true},"outputs":[],"source":["class ResidualLSTM(nn.Module):\n","    \n","    def __init__(self, d_model):\n","        super(ResidualLSTM, self).__init__()\n","        self.LSTM=nn.LSTM(d_model, d_model, num_layers=1, bidirectional=True)\n","        self.linear1=nn.Linear(d_model*2, d_model*4)\n","        self.linear2=nn.Linear(d_model*4, d_model)\n","        \n","\n","            \n","    def forward(self, x):\n","        res=x\n","        x, _ = self.LSTM(x)\n","        x=F.relu(self.linear1(x))\n","        x=self.linear2(x)\n","        x=res+x\n","        return x\n","    \n","    \n","class SAKTModel(nn.Module):\n","    \n","    \n","    \n","    def _init_weights(self, module):\n","        \n","        init_std = 0.05\n","        \n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=init_std)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","    \n","    \n","    def __init__(self, nout, n_embd=128,  nlayers=3, rnnlayers=3,\n","    dropout=0.1, nheads=8, target_phi=False):\n","        super(SAKTModel, self).__init__()\n","        self.n_embd = n_embd\n","        self.pos_encoder = nn.ModuleList([ResidualLSTM(n_embd) for i in range(rnnlayers)])\n","        \n","        self.pos_encoder_dropout = nn.Dropout(dropout)\n","        \n","        self.target_phi = target_phi\n","        if self.target_phi is False:\n","            self.embedding = nn.Linear(8, n_embd)\n","        else:\n","            self.embedding = nn.Linear(7, n_embd)\n","        self.layer_normal = nn.LayerNorm(n_embd)\n","        encoder_layers = [nn.TransformerEncoderLayer(n_embd, nheads, n_embd*4, dropout) for i in range(nlayers)]\n","        conv_layers = [nn.Conv1d(n_embd,n_embd,(nlayers-i)*2-1,stride=1,padding=0) for i in range(nlayers)]\n","        deconv_layers = [nn.ConvTranspose1d(n_embd,n_embd,(nlayers-i)*2-1,stride=1,padding=0) for i in range(nlayers)]\n","        layer_norm_layers = [nn.LayerNorm(n_embd) for i in range(nlayers)]\n","        layer_norm_layers2 = [nn.LayerNorm(n_embd) for i in range(nlayers)]\n","        self.transformer_encoder = nn.ModuleList(encoder_layers)\n","        self.conv_layers = nn.ModuleList(conv_layers)\n","        self.layer_norm_layers = nn.ModuleList(layer_norm_layers)\n","        self.layer_norm_layers2 = nn.ModuleList(layer_norm_layers2)\n","        self.deconv_layers = nn.ModuleList(deconv_layers)\n","        self.nheads = nheads\n","        self.pred = nn.Linear(n_embd, nout)\n","        if self.target_phi:\n","            self.pred2 = nn.Linear(nout, 2)\n","\n","\n","        self.apply(self._init_weights)\n","            \n","\n","    def forward(self, numerical_features, has_event):\n","        B, T, C = numerical_features.shape\n","        x=self.embedding(numerical_features.view(B*T, C)).view(B, T, self.n_embd)\n","        \n","        # expand dims of has_event\n","        has_event = has_event.unsqueeze(-1)\n","        x = x * has_event\n","        # x = x.permute(1, 0, 2)\n","        # for lstm in self.pos_encoder:\n","        #     lstm.LSTM.flatten_parameters()\n","        #     x=lstm(x)\n","\n","        # x = self.pos_encoder_dropout(x)\n","        # x = self.layer_normal(x)\n","\n","\n","\n","        # for conv, transformer_layer, layer_norm1, layer_norm2, deconv in zip(self.conv_layers,\n","        #                                                        self.transformer_encoder,\n","        #                                                        self.layer_norm_layers,\n","        #                                                        self.layer_norm_layers2,\n","        #                                                        self.deconv_layers):\n","        #     #LXBXC to BXCXL\n","        #     res=x\n","        #     x=F.relu(conv(x.permute(1,2,0)).permute(2,0,1))\n","        #     x=layer_norm1(x) / 8\n","        #     x=transformer_layer(x) / 8\n","        #     x=F.relu(deconv(x.permute(1,2,0)).permute(2,0,1))\n","        #     x=layer_norm2(x) / 8\n","        #     x=res+x\n","\n","        # x = x.permute(1, 0, 2)\n","        \n","        res = x\n","        x = x.permute(1, 0, 2)\n","        for lstm in self.pos_encoder:\n","            lstm.LSTM.flatten_parameters()\n","            x=lstm(x)\n","        x = x.permute(1, 0, 2)\n","        x = self.pos_encoder_dropout(x)\n","        x = x * has_event\n","        # x = self.pos_encoder_dropout(x)\n","        x = self.layer_normal(x) / 8\n","        \n","    \n","        for transformer_layer, layer_norm1, layer_norm2 in zip(self.transformer_encoder, self.layer_norm_layers, self.layer_norm_layers2):\n","            res=x\n","            x=layer_norm1(x) / 8\n","            x=transformer_layer(x) / 8\n","            x = x * has_event\n","            x=layer_norm2(x) / 8\n","            x=res+x\n","\n","    \n","        # x = x[:, 0, :]\n","        x = torch.sum(x, axis=1)*4 / torch.sum(has_event, axis=1) # (B, T, C) -> (B, C))\n","        \n","        \n","        x = self.pred(x)\n","        \n","        \n","        \n","        \n","        x = x.squeeze(-1)\n","        \n","        # (B, T, C) -> (B, C)\n","        if self.target_phi is False:\n","            print(x.var().item())\n","            output = F.softmax(x, dim=1)\n","        else:\n","            print(x.var().item())\n","            output = self.pred2(x)\n","            # normalize output; output is a xy vector\n","            output = output / torch.norm(output, dim=1, keepdim=True)\n","        \n","        return output"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[-1.2247e+00,  0.0000e+00,  1.2247e+00],\n","        [-1.2247e+00,  1.1921e-07,  1.2247e+00],\n","        [ 0.0000e+00,  0.0000e+00,  0.0000e+00]],\n","       grad_fn=<NativeLayerNormBackward0>)"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import torch.nn as nn\n","layer = nn.LayerNorm(3)\n","x = torch.tensor([[0, 1, 2], [2, 3, 4], [0, 0, 0]], dtype=torch.float32)\n","layer(x)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["class NeutrinoDetection(nn.Module):\n","    def __init__(self):\n","        super(NeutrinoDetection, self).__init__()\n","        self.model_theta = SAKTModel(nout=num_bins, n_embd=n_embd,  nlayers=3, rnnlayers=1, dropout=dropout, nheads=num_heads).to(device)\n","        self.model_phi = SAKTModel(nout=num_bins, n_embd=n_embd,  nlayers=1, rnnlayers=1, dropout=dropout, nheads=num_heads, target_phi=True).to(device)\n","        \n","        \n","        \n","\n","    def forward(self, X, targets=None):\n","        \n","        xyz, time, charge, charge_clipped, auxiliary, has_event = X[\"xyz\"], X[\"time\"], X[\"charge\"], X[\"charge_clipped\"], X[\"auxiliary\"], X[\"has_event\"]\n","        \n","        x_theta = torch.cat([xyz, time.unsqueeze(-1), charge.unsqueeze(-1), charge_clipped.unsqueeze(-1), auxiliary.unsqueeze(-1), has_event.unsqueeze(-1)], dim=-1)\n","        \n","        # take only x and y coordinates\n","        x_phi = torch.cat([xyz[:, :, :2], time.unsqueeze(-1), charge.unsqueeze(-1), charge_clipped.unsqueeze(-1), auxiliary.unsqueeze(-1), has_event.unsqueeze(-1)], dim=-1)\n","        \n","        \n","        theta_probas = self.model_theta(x_theta, has_event)\n","        xy_vec = self.model_phi(x_phi, has_event)\n","        \n","        \n","        \n","        theta_pred = np.pi * torch.argmax(theta_probas, dim=1) / num_bins # (B)\n","        # phi_pred = 2 * np.pi * torch.argmax(phi_probas, dim=1) / num_bins # (B)\n","        \n","        # get phi from xy_vec (B, 2)\n","        phi_pred = torch.atan2(xy_vec[:, 1], xy_vec[:, 0])\n","        # make sure phi is in [0, 2pi]\n","        phi_pred = torch.where(phi_pred < 0, phi_pred + 2*np.pi, phi_pred)\n","        phi_pred = torch.where(phi_pred > 2*np.pi, phi_pred - 2*np.pi, phi_pred)\n","\n","        \n","        \n","        # define the bin edges for the ordinal labels\n","        # bin_edges = torch.linspace(0, 1, num_bins+1)[1:-1].to(device)\n","        \n","        # cumprods_theta = torch.cumprod(theta_probas > 0.5, dim=1)\n","        # theta_pred = (torch.sum(cumprods_theta, dim=1) - 1) * np.pi / num_bins # (B)\n","        # cumprods_phi = torch.cumprod(phi_probas > 0.5, dim=1)\n","        # phi_pred = (torch.sum(cumprods_phi, dim=1) - 1) * 2*np.pi / num_bins # (B)\n","        \n","        if targets is None:\n","            loss = None\n","            loss_angular = None\n","        else: \n","            # Get the angle theta and phi from the 3D vector\n","            phi_target = targets[:, 0]\n","            theta_target = targets[:, 1]\n","            \n","            \n","            \n","            \n","            \n","            losses_angular = angular_dist_score(phi_target, theta_target, phi_pred, theta_pred)\n","            # losses_angular = angular_dist_score(phi_target, theta_target, phi, theta)\n","            # loss_angular = torch.sum(event_size * losses_angular) / torch.sum(event_size)\n","            loss_angular = torch.mean(losses_angular)\n","            \n","            \n","            \n","            \n","            # construct the phi_target_probability and theta_target_probability: these are one-hot vectors with the same shape as phi_probas and theta_probas, with 1 at the position of \n","            theta_target = theta_target / np.pi\n","            theta_target = theta_target * num_bins\n","            theta_target = theta_target.long()\n","            theta_target = torch.nn.functional.one_hot(theta_target, num_classes=num_bins)\n","            \n","            # phi_target = phi_target / (2*np.pi)\n","            # phi_target = phi_target * num_bins\n","            # phi_target = phi_target.long()\n","            # phi_target = torch.nn.functional.one_hot(phi_target, num_classes=num_bins)\n","            \n","            \n","\n","            # loss is sum of the cross entropy of the predicted probabilities and the target probabilities\n","            eps = 1e-8\n","            loss_theta = torch.mean(-torch.sum(torch.log(theta_probas+eps)*theta_target, axis=-1))\n","            # loss_phi = torch.mean(-torch.sum(torch.log(phi_probas+eps)*phi_target, axis=-1))\n","            \n","            # construct xy_target from phi_target \n","            xy_target = torch.zeros((len(phi_target), 2), dtype=torch.float32).to(device)\n","            xy_target[:, 0] = torch.cos(phi_target)\n","            xy_target[:, 1] = torch.sin(phi_target)\n","            # loss_phi is squared distance between xy_target and xy_vec\n","            loss_phi = torch.mean(torch.sum((xy_target - xy_vec)**2, axis=-1))\n","            \n","            \n","            \n","            loss = loss_theta + 2*loss_phi\n","            # loss = loss_phi\n","            \n","            \n","\n","            # # construct the theta_target_probability and phi_target_probability as ordinal targets\n","            # theta_target_bins = torch.bucketize(theta_target, bin_edges).to(device)\n","            # theta_target_probability = torch.zeros((len(theta_target), num_bins), dtype=torch.float32)\n","            # for i, bin_idx in enumerate(theta_target_bins):\n","            #     theta_target_probability[i, :bin_idx] = 1\n","\n","            # phi_target_bins = torch.bucketize(phi_target, bin_edges).to(device)\n","            # phi_target_probability = torch.zeros((len(phi_target), num_bins), dtype=torch.float32)\n","            # for i, bin_idx in enumerate(phi_target_bins):\n","            #     phi_target_probability[i, :bin_idx] = 1\n","\n","            # # send to device\n","            # theta_target_probability = theta_target_probability.to(device)\n","            # phi_target_probability = phi_target_probability.to(device)\n","            \n","            # # compute the ordinal loss\n","            # eps = 1e-8\n","            # loss_theta = torch.mean(-torch.sum(torch.log(torch.cat((theta_probas, 1-theta_probas), dim=-1)+eps) * torch.cat((theta_target_probability, 1-theta_target_probability), dim=-1), axis=-1))\n","            # loss_phi = torch.mean(-torch.sum(torch.log(torch.cat((phi_probas, 1-phi_probas), dim=-1)+eps) * torch.cat((phi_target_probability, 1-phi_target_probability), dim=-1), axis=-1))\n","            # loss = loss_theta + loss_phi\n","\n","\n","            \n","            \n","            \n","        \n","        return (x, loss), (theta_pred, phi_pred, loss_angular)\n","        "]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["1.9994182630925503"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","phi = np.random.uniform(0, 2*np.pi, 100000)\n","y = np.sin(phi)\n","x = np.cos(phi)\n","np.average((x-1)**2 + (y-0)**2)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["model = NeutrinoDetection()"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0.026182521134614944\n","0.02658679150044918\n"]}],"source":["X, y = data_loader.get_xy('train')\n","x = model(X)\n"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# x = x.detach().cpu().numpy().flatten()\n","# np.std(x)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAD5CAYAAAAuneICAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAASgElEQVR4nO3df6xc513n8feHpA0LFJpsroNrOzhFDpKDRNu9eJEKqDSIBlLh/JPKCJABSxYoS+lqV8ShfyBWsuS2qPwQdFdW2l1X/ZEa2ioWBdrUEH5Ibcx1Nm3jpCHexpsYe2NTioD9I6zd7/4xJ9H05v6YmTtzZ+593i/pas6cec7M9849dz73eZ5zzk1VIUlqzzdNuwBJ0nQYAJLUKANAkhplAEhSowwASWqUASBJjbp2kEZJXg3cD3wvUMAvAE8BHwN2AueAt1XV17r29wEHgKvA26vq0ys9/4033lg7d+4cpX5Jatbp06f/vqrmRt0+g5wHkOQY8FdVdX+SVwLfAvwa8A9VdSTJIeD6qro3yW7go8Ae4DXAZ4Fbq+rqcs8/Pz9fCwsLo34PktSkJKeran7U7VcdAkry7cAPA+8HqKp/rap/BPYCx7pmx4C7uuW9wANV9UJVPQOcpRcGkqQZMsgcwGuBy8B/T/I/k9yf5FuBm6rqIkB3u6Vrvw14rm/78906SdIMGSQArgXeAPzXqno98H+BQyu0zxLrXjbOlORgkoUkC5cvXx6oWEnS+AwSAOeB81X1SHf/D+kFwvNJtgJ0t5f62u/o2347cGHxk1bV0aqar6r5ubmR5zAkSSNaNQCq6v8AzyX5nm7V7cATwAlgf7duP/Bgt3wC2JfkuiS3ALuAU2OtWpK0ZgMdBgr8MvDh7gigrwA/Ty88jic5ADwL3A1QVWeSHKcXEleAe1Y6AkiSNB0DBUBVPQYsdajR7cu0PwwcHr0sSdKkeSawJDXKAJCkRg06ByBN1c5Dn3pp+dyRO6dYibR52AOQpEYZAJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMMAElqlAEgSY0yACSpUQaAJDXKAJCkRhkAktQoA0CSGmUASFKjDABJapQBIEmNunbaBUjD2nnoUy8tnzty5xQrkTY2ewCS1KiBAiDJuSRfSvJYkoVu3Q1JHkrydHd7fV/7+5KcTfJUkrdMqnhJ0uiG6QH8SFW9rqrmu/uHgJNVtQs42d0nyW5gH3AbcAfwviTXjLFmSdIYrGUOYC/wpm75GPAwcG+3/oGqegF4JslZYA/wuTW8lhrUP9YvafwG7QEU8Jkkp5Mc7NbdVFUXAbrbLd36bcBzfdue79ZJkmbIoD2AN1bVhSRbgIeSfHmFtlliXb2sUS9IDgLcfPPNA5YhSRqXgXoAVXWhu70EfJLekM7zSbYCdLeXuubngR19m28HLizxnEerar6q5ufm5kb/DiRJI1k1AJJ8a5JXvbgM/BjwOHAC2N812w882C2fAPYluS7JLcAu4NS4C5ckrc0gQ0A3AZ9M8mL7j1TVnyb5G+B4kgPAs8DdAFV1Jslx4AngCnBPVV2dSPWSpJGtGgBV9RXg+5ZY/1Xg9mW2OQwcXnN1kqSJ8UxgSWqUASBJjfJicNrQvDCcNDoDQDPFs3+l9eMQkCQ1ygCQpEY5BKSpc9hHmg57AJLUKANAkhplAEhSowwASWqUASBJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIa5aUgtGl4aWhpOPYAJKlRBoAkNcohIK0bh2ik2WIPQJIaZQ9Am9Li/zFgj0N6OXsAktQoA0CSGuUQkKbCfwMpTd/AAZDkGmAB+LuqemuSG4CPATuBc8DbquprXdv7gAPAVeDtVfXpMdetGeMRPtLGM8wQ0K8AT/bdPwScrKpdwMnuPkl2A/uA24A7gPd14SFJmiEDBUCS7cCdwP19q/cCx7rlY8BdfesfqKoXquoZ4CywZyzVSpLGZtAewG8Dvwp8vW/dTVV1EaC73dKt3wY819fufLdOkjRDVp0DSPJW4FJVnU7ypgGeM0usqyWe9yBwEODmm28e4Gm1ETnZK82uQXoAbwR+Msk54AHgzUk+BDyfZCtAd3upa38e2NG3/XbgwuInraqjVTVfVfNzc3Nr+BYkSaNYNQCq6r6q2l5VO+lN7v5ZVf0McALY3zXbDzzYLZ8A9iW5LsktwC7g1NgrlyStyVrOAzgCHE9yAHgWuBugqs4kOQ48AVwB7qmqq2uuVJI0VkMFQFU9DDzcLX8VuH2ZdoeBw2usTZI0QV4KQpIa5aUgGuRZu5LAHoAkNcsAkKRGOQSkVTlkJG1OBoDGzrN/pY3BAGjEch/K/nUvtcs5AElqlAEgSY0yACSpUQaAJDXKSWC9ZDNPCG/m700alT0ASWqUPQCNbKMe729vQOqxByBJjTIAJKlRqXrZ/2tfd/Pz87WwsDDtMja1jTpcs54cDtJGk+R0Vc2Pur09AElqlAEgSY0yACSpUQaAJDXK8wA2MSd+Ja3EHoAkNcoAkKRGGQCS1CgDQJIaZQBIUqNWDYAk35zkVJIvJDmT5De69TckeSjJ093t9X3b3JfkbJKnkrxlkt+AJGk0g/QAXgDeXFXfB7wOuCPJDwCHgJNVtQs42d0nyW5gH3AbcAfwviTXTKB2SdIarBoA1fMv3d1XdF8F7AWOdeuPAXd1y3uBB6rqhap6BjgL7Bln0ZKktRtoDiDJNUkeAy4BD1XVI8BNVXURoLvd0jXfBjzXt/n5bp0kaYYMFABVdbWqXgdsB/Yk+d4Vmmepp3hZo+RgkoUkC5cvXx6oWEnS+Ax1FFBV/SPwML2x/eeTbAXobi91zc4DO/o22w5cWOK5jlbVfFXNz83NDV+5JGlNBjkKaC7Jq7vlfwP8KPBl4ASwv2u2H3iwWz4B7EtyXZJbgF3AqTHXLU3UzkOfeulL2qwGuRjcVuBYdyTPNwHHq+qPknwOOJ7kAPAscDdAVZ1Jchx4ArgC3FNVVydTviRpVKsGQFV9EXj9Euu/Cty+zDaHgcNrrk6SNDGeCSxJjTIAJKlRBoAkNcoAkKRGGQCS1CgDQJIaZQBIUqMGORFMG4hnrkoalD0ASWqUASBJjXIISOo4fKbW2AOQpEbZA5BW0d8zOHfkzilWIo2XPQBJapQBIEmNMgAkqVEGgCQ1ygCQpEYZAJLUKANAkhplAEhSowwASWqUASBJjfJSENIQvCyENhN7AJLUKANAkhplAEhSo1adA0iyA/gg8J3A14GjVfU7SW4APgbsBM4Bb6uqr3Xb3AccAK4Cb6+qT0+kemmKlvsHMs4NaKMYZBL4CvCfqurRJK8CTid5CPg54GRVHUlyCDgE3JtkN7APuA14DfDZJLdW1dXJfAuzZz0nCv0vVpJGteoQUFVdrKpHu+V/Bp4EtgF7gWNds2PAXd3yXuCBqnqhqp4BzgJ7xly3JGmNhpoDSLITeD3wCHBTVV2EXkgAW7pm24Dn+jY7362TJM2QgQMgybcBHwfeUVX/tFLTJdbVEs93MMlCkoXLly8PWoYkaUwGCoAkr6D34f/hqvpEt/r5JFu7x7cCl7r154EdfZtvBy4sfs6qOlpV81U1Pzc3N2r9kqQRDXIUUID3A09W1Xv7HjoB7AeOdLcP9q3/SJL30psE3gWcGmfRs8KzQiVtZIMcBfRG4GeBLyV5rFv3a/Q++I8nOQA8C9wNUFVnkhwHnqB3BNE9LR0BJEkbxaoBUFV/zdLj+gC3L7PNYeDwGuqSJE2YZwJLUqO8GuiYeEKWpI3GHoAkNcoAkKRGOQQ0YR4qKmlWGQDryDCQNEsMAGnMDHptFM4BSFKjDABJapQBIEmNMgAkqVFOAktT4ESxZoE9AElqlD2ADcjrDm0c/qWvWWYPQJIaZQBIUqMMAElqlHMAU+LYsKRpswcgSY0yACSpUQ4BSVPmcKCmxQCQZohhoPXkEJAkNcoegLROPINbs8YAGNK0fon98JA0bg4BSVKjVg2AJB9IcinJ433rbkjyUJKnu9vr+x67L8nZJE8lecukCt9Mdh761EtfkrReBhkC+h/A7wEf7Ft3CDhZVUeSHOru35tkN7APuA14DfDZJLdW1dXxlr2+/GCWtBmtGgBV9ZdJdi5avRd4U7d8DHgYuLdb/0BVvQA8k+QssAf43JjqlZrhIaGatFEngW+qqosAVXUxyZZu/Tbg833tznfrNAJ7HpImadyTwFliXS3ZMDmYZCHJwuXLl8dchiRpNaP2AJ5PsrX7638rcKlbfx7Y0dduO3BhqSeoqqPAUYD5+fklQ0JSj8NBmoRRA+AEsB840t0+2Lf+I0neS28SeBdwaq1FtsRhH0nrZdUASPJRehO+NyY5D/w6vQ/+40kOAM8CdwNU1Zkkx4EngCvAPRv9CCBJ2qwGOQrop5Z56PZl2h8GDq+lKEnLczhI4+KZwJLUKK8FJG1g9ga0FvYAJKlRBoAkNcoAkKRGOQcgbRLOB2hY9gAkqVH2AKRNzp6BlmMASJuQlxTRIBwCkqRG2QNYhn9BqQWD7OcOG21e9gAkqVH2ACQNzAnlzcUAkBoyytCmw6GblwEgaV3Ye5g9BoCkmWd4TIYBIGkkk/5QXm7oyTAYHwOgj2Od0miW+91Z7gPaD/HZ4GGgktQoewCSJsZe9WwzACRN1VqGgxxKWhuHgCSpUfYAJG0K9gaGZw9AkhplD0DSzHDSeH0ZAJI2nWHPS2iVQ0CS1KiJ9QCS3AH8DnANcH9VHZnUa62FXU6pHctNFC/+HGilpzCRHkCSa4DfB34c2A38VJLdk3gtSdJoJtUD2AOcraqvACR5ANgLPDGh1xuKf/VLWulzYJCewmboJUwqALYBz/XdPw/8+wm91kA/FD/0JY1ikKuSrsU0g2RSAZAl1tU3NEgOAge7u/+S5KmxvPC7Vnz4RuDvx/E6Y2Zdw7Gu4VjXcNa1rlU+s/otVdd3reW1JxUA54Edffe3Axf6G1TVUeDohF5/SUkWqmp+PV9zENY1HOsajnUNp6W6JnUY6N8Au5LckuSVwD7gxIReS5I0gon0AKrqSpL/AHya3mGgH6iqM5N4LUnSaCZ2HkBV/THwx5N6/hGt65DTEKxrONY1HOsaTjN1papWbyVJ2nS8FIQkNWrDBkCSO5I8leRskkNLPJ4kv9s9/sUkb+h77ANJLiV5fNE2NyR5KMnT3e31M1LXe5J8uWv/ySSvnoW6+h7/z0kqyY2zUleSX+6e90ySd89CXUlel+TzSR5LspBkz3rVlWRHkj9P8mT3nvxK3zZT2+9XqWtq+/1KdfVtu+77/Wp1Db3fV9WG+6I3sfy/gNcCrwS+AOxe1OYngD+hd07CDwCP9D32w8AbgMcXbfNu4FC3fAh414zU9WPAtd3yu2alru6xHfQm+/83cOMs1AX8CPBZ4Lru/pYZqeszwI/3bf/wetUFbAXe0C2/CvjbF7ed5n6/Sl1T2+9Xqmua+/0q79fQ+/1G7QG8dKmJqvpX4MVLTfTbC3ywej4PvDrJVoCq+kvgH5Z43r3AsW75GHDXLNRVVZ+pqivd3c/TO69i6nV1fgv4VRad6Dflun4JOFJVL3TtLs1IXQV8e7f8HSw6N2aSdVXVxap6tKvvn4En6Z2x/+I2U9nvV6prmvv9Ku8XTGm/X6Wuoff7jRoAS11qYtsIbRa7qaouAnS3W2akrn6/QO8vg6nXleQngb+rqi8MWc9E6wJuBX4oySNJ/iLJ989IXe8A3pPkOeA3gfumUVeSncDrgUe6VTOx3y9RV7+p7feL65qV/X6J92vo/X6j/kOYVS81MWCbcZtoXUneCVwBPjztupJ8C/BOet30UU3q/boWuJ5e1/n7geNJXltdv3iKdf0S8B+r6uNJ3ga8H/jRAWsaS11Jvg34OPCOqvqnIV57anVNc79fXNes7PfLvF9D7/cbtQew6qUmBmyz2PMvduO722GHDiZVF0n2A28FfnqID7JJ1vXdwC3AF5Kc69o/muQ7p1zXi9t8ous+nwK+Tu86KtOuaz/wiW75D+gNBQxjTXUleQW9D40PV9Un+tpMdb9foa6p7vfL1DX1/X6F92v4/X6lCYJZ/aKXdF/pfhAvTqLctqjNnXzjJMqpRY/v5OWTdO/hGyfD3j0jdd1B71Lac7P0fi16/BzDT4ZN6v36ReC/dMu30utKZwbqehJ4U7d8O3B6vd6v7v4Hgd9e4nmntt+vUtfU9vuV6prmfr/K+zX0fj/0mzorX/Rmyf+W3mz6O/vegF/se6N+v3v8S8B837YfBS4C/49eah7o1v9b4CTwdHd7w4zUdbb7YT7Wff23Wahrrb8IE3y/Xgl8CHgceBR484zU9YPAaXq/8I8A/2696upeu4Av9u1HPzHt/X6Vuqa2369U1zT3+1Xer6H3e88ElqRGbdQ5AEnSGhkAktQoA0CSGmUASFKjDABJapQBIEmNMgAkqVEGgCQ16v8Dn0fzT+k1CswAAAAASUVORK5CYII=","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# plt.hist(x.detach().cpu().numpy().flatten(), bins=100);\n","# # plt.hist(res.detach().cpu().numpy().flatten(), bins=100);\n","# # plt.xlim(-0.5, 0.5)"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-02-26T16:51:32.653443Z","iopub.status.busy":"2023-02-26T16:51:32.653152Z","iopub.status.idle":"2023-02-26T16:51:32.659362Z","shell.execute_reply":"2023-02-26T16:51:32.658269Z","shell.execute_reply.started":"2023-02-26T16:51:32.653417Z"},"trusted":true},"outputs":[],"source":["# start a new wandb run to track this script\n","if LOG_WANDB:\n","    # from kaggle_secrets import UserSecretsClient\n","    # user_secrets = UserSecretsClient()\n","    # secret_value = user_secrets.get_secret(\"wandb_key\")\n","    \n","    wandb.login(key=\"df71c3860e28082a4ef9322a8970ab548124177e\")\n","    \n","    wandb.init(\n","        # set the wandb project where this run will be logged\n","        project=\"neutrino-detection\",\n","\n","        # track hyperparameters and run metadata\n","        config={\n","        \"n_layers\":n_layers,\n","        \"num_heads\":num_heads\n","        }\n","    )\n","    "]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# import torch\n","# x = torch.tensor([[0, 0, 1], [1, 1, 20]], dtype=torch.float32)\n","# x - x.mean(axis=0, keepdims=True), x.std(axis=0, keepdims=True), (x - x.mean(axis=0, keepdims=True)) / x.std(axis=0, keepdims=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-02-26T16:51:32.661428Z","iopub.status.busy":"2023-02-26T16:51:32.660821Z","iopub.status.idle":"2023-02-26T16:55:18.707639Z","shell.execute_reply":"2023-02-26T16:55:18.706038Z","shell.execute_reply.started":"2023-02-26T16:51:32.661393Z"},"trusted":true},"outputs":[],"source":["\n","# create a PyTorch optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","best_loss_angular = 1e10\n","total_loss = 0\n","total_loss_angular = 0\n","\n","\n","for iter in tqdm(range(max_iters)):\n","\n","\n","\n","    if iter % gradient_accumulation_steps ==  0 :\n","        optimizer.zero_grad(set_to_none=True)\n","        total_loss *= 1 / gradient_accumulation_steps\n","        total_loss_angular *= 1 / gradient_accumulation_steps\n","        print(f\"step {iter}: train distance loss {total_loss:.4f}, train angular loss {total_loss_angular:.4f} \")\n","        if LOG_WANDB and iter > 0 and iter % print_each == 0:\n","            wandb.log({\"dis_loc\": total_loss,\n","                        \"ang_loc\": total_loss_angular\n","                        })\n","            # wandb.log({\"model\": model.state_dict()})\n","        total_loss = 0.\n","        total_loss_angular = 0.\n","    if iter % print_model_each == 0:\n","        indx = iter // print_model_each\n","        torch.save(model.state_dict(), f'model_last_overfit_on_batch_{indx}.pth')  \n","    \n","    \n","    \n","    X, y = data_loader.get_xy('train')\n","    (x, loss), (theta, phi, loss_angular) = model(X, y)\n","\n","    total_loss += loss.item()\n","    total_loss_angular += loss_angular.item()\n","    loss.backward()\n","    \n","    \n","        \n","    # if grad_clip:\n","    #     torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_val)\n","    \n","\n","    # Update the weights every gradient_accumulation_steps batches\n","    if iter % gradient_accumulation_steps == gradient_accumulation_steps - 1:\n","        optimizer.step()\n","    \n","    \n","\n","if LOG_WANDB:\n","    wandb.finish()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-02-26T16:55:18.713419Z","iopub.status.idle":"2023-02-26T16:55:18.714188Z","shell.execute_reply":"2023-02-26T16:55:18.713929Z","shell.execute_reply.started":"2023-02-26T16:55:18.713905Z"},"trusted":true},"outputs":[],"source":["# df = df.sort_values([\"event_id\"])\n","# df.to_csv('submission.csv', index=False)\n","# df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.13"},"vscode":{"interpreter":{"hash":"4dbc9917bcaa9a9fa434c727723b90f93ecc3435121eacd019fcd02c268a833c"}}},"nbformat":4,"nbformat_minor":4}
